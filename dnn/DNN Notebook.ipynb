{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00a91b73a105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# from utils import augment_EEG, cart2sph, pol2cart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "# np.random.seed(1234)\n",
    "from functools import reduce\n",
    "import math as m\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.preprocessing import scale\n",
    "# from utils import augment_EEG, cart2sph, pol2cart\n",
    "\n",
    "# import DNN frameworks\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# import high level optimizers, models and layers\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "# for CNN\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# for RNN\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# for utility functionality\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "# imports tensorflow\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "3\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print len(mnist)\n",
    "print type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Test Building the CNN\n",
    "\n",
    "Here we want to create a function to build each layer of a CNN, say VGG style, or updated to be latest state-of-the art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adjmats', 'winsize', 'timepoints', 'includedchans', 'stepsize']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "testfile = os.path.join('/Users/adam2392/Documents/pydata/output/mvar/la11_ictal/la11_ictal_mvarmodel.npz')\n",
    "data = np.load(testfile)\n",
    "print data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "n_colors = 1\n",
    "imsize=28\n",
    "\n",
    "X_train= mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "# y_train=keras.utils.np_utils.to_categorical(y_train, num_classes=10)\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# reshape into correct size to feed into NN\n",
    "X_train = np.reshape(X_train, [-1, imsize, imsize, n_colors])\n",
    "X_test = np.reshape(X_test, [-1, imsize, imsize, n_colors])\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im size is :  28\n"
     ]
    }
   ],
   "source": [
    "n_colors=1\n",
    "imsize=X_train.shape[1]\n",
    "print \"im size is : \", imsize\n",
    "\n",
    "def build_cnn(model, w_init=None, n_layers=(4,2,1),poolsize=(2,2),n_filters_first=32):    \n",
    "    DEBUG=0\n",
    "    \n",
    "    # check for weight initialization -> apply Glorotuniform\n",
    "    if w_init is None:\n",
    "        w_init = [keras.initializers.glorot_uniform()] * sum(n_layers)\n",
    "    \n",
    "    # set up input layer of CNN\n",
    "    model.add(InputLayer(input_shape=(imsize, imsize, n_colors)))\n",
    "    # initialize counter\n",
    "    count=0\n",
    "    \n",
    "    # add the rest of the hidden layers\n",
    "    for idx, n_layer in enumerate(n_layers):\n",
    "        for ilay in range(n_layer):\n",
    "            model.add(Conv2D(n_filters_first*(2 ** idx), \n",
    "                             (3, 3),\n",
    "                             input_shape=(imsize, imsize, n_colors),\n",
    "                             kernel_initializer=w_init[count], activation='relu'))\n",
    "            if DEBUG:\n",
    "                print model.output_shape\n",
    "                print idx, \" and \", ilay\n",
    "            count+=1\n",
    "\n",
    "        # create a network at the end with a max pooling\n",
    "        model.add(MaxPooling2D(pool_size=poolsize))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max_pooling2d_33/MaxPool:0\", shape=(?, 5, 5, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w_init=None\n",
    "model = Sequential()\n",
    "model = build_cnn(model, w_init=w_init, n_layers=(2,1), n_filters_first=16)\n",
    "print model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 800)\n",
      "(None, 800)\n",
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "# to test training of this network, add a softmax at the end\n",
    "model.add(Flatten())\n",
    "print model.output_shape\n",
    "model.add(Dense(800, activation='relu'))\n",
    "print model.output_shape\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "print model.output_shape\n",
    "\n",
    "# Set Optimizer\n",
    "# Nesterov Stochastic Gradient Descent\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# ADAM\n",
    "ADAM = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ADAM)\n",
    "\n",
    "# cnn_config = model.get_config()\n",
    "# display(cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(10000, 784)\n",
      "Tensor(\"dense_38/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 2.2494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1309b3bd0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_colors = 1\n",
    "imsize = 28\n",
    "\n",
    "X_train= mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "\n",
    "# reshape into correct size to feed into NN\n",
    "x_train = np.reshape(X_train, [-1, imsize, imsize, n_colors])\n",
    "x_test = np.reshape(X_test, [-1, imsize, imsize, n_colors])\n",
    "\n",
    "print model.output\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape\n",
    "\n",
    "print type(X_train)\n",
    "print type(y_train)\n",
    "\n",
    "# model.fit(X_train, y_train, batch_size=32, epochs=1)\n",
    "model.fit(x_train[:100,:,:,:], y_train[:100,:], batch_size=20, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Second Test Building The LSTM\n",
    "\n",
    "We now want to build the RNN layers using LSTM. First show an example of the LSTM running sequence classification on the imdb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 95s 5us/step\n",
      "17473536/17464789 [==============================] - 95s 5us/step\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000,)\n",
      "(25000, 500)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 307s 12ms/step - loss: 0.5010 - acc: 0.7522 - val_loss: 0.3487 - val_acc: 0.8565\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 309s 12ms/step - loss: 0.4031 - acc: 0.8195 - val_loss: 0.3489 - val_acc: 0.8559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a824310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.59%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi GPU Training Set Up\n",
    "\n",
    "Using multiple gpus and/or just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_convpool_lstm(model, nb_classes, grad_clip=110, imsize=32, n_colors=3, n_timewin=3):\n",
    "    \"\"\"\n",
    "    Builds the complete network with LSTM layer to integrate time from sequences of EEG images.\n",
    "\n",
    "    :param input_vars: list of EEG images (one image per time window)\n",
    "    :param nb_classes: number of classes\n",
    "    :param grad_clip:  the gradient messages are clipped to the given value during\n",
    "                        the backward pass.\n",
    "    :param imsize: size of the input image (assumes a square input)\n",
    "    :param n_colors: number of color channels in the image\n",
    "    :param n_timewin: number of time windows in the snippet\n",
    "    :return: a pointer to the output of last layer\n",
    "    \"\"\"\n",
    "    convnets = []\n",
    "    w_init = None\n",
    "    \n",
    "    # Build 7 parallel CNNs with shared weights\n",
    "    for i in range(n_timewin):\n",
    "        if i == 0:\n",
    "            convnet, w_init = build_cnn(input_vars[i], imsize=imsize, n_colors=n_colors)\n",
    "        else:\n",
    "            convnet, _ = build_cnn(input_vars[i], w_init=w_init, imsize=imsize, n_colors=n_colors)\n",
    "        convnets.append(FlattenLayer(convnet))\n",
    "    # at this point convnets shape is [numTimeWin][n_samples, features]\n",
    "    # we want the shape to be [n_samples, features, numTimeWin]\n",
    "    convpool = ConcatLayer(convnets)\n",
    "    convpool = ReshapeLayer(convpool, ([0], n_timewin, get_output_shape(convnets[0])[1]))\n",
    "    # Input to LSTM should have the shape as (batch size, SEQ_LENGTH, num_features)\n",
    "    convpool = LSTMLayer(convpool, num_units=128, grad_clipping=grad_clip,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    # We only need the final prediction, we isolate that quantity and feed it\n",
    "    # to the next layer.\n",
    "    convpool = SliceLayer(convpool, -1, 1)      # Selecting the last prediction\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
    "            num_units=256, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    # And, finally, the output layer with 50% dropout on its inputs:\n",
    "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
    "            num_units=nb_classes, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return convpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-523c8b687668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# make the model parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn' is not defined"
     ]
    }
   ],
   "source": [
    "# number of gpus\n",
    "G = 1\n",
    "# we'll store a copy of the model on *every* GPU and then combine\n",
    "# the results from the gradient updates on the CPU\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # initialize the model\n",
    "    model = cnn.build(width=32, height=32, depth=3, classes=10)\n",
    "\n",
    "# make the model parallel\n",
    "model = multi_gpu_model(model, gpus=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<contextlib.GeneratorContextManager object at 0x10a994910>\n"
     ]
    }
   ],
   "source": [
    "print tf.device(\"/cpu:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow GPU",
   "language": "python",
   "name": "tensorflowgpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
